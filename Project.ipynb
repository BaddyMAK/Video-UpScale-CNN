{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\bmakh\\anaconda3\\envs\\upvideo\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "# print (sys.version)\n",
    "import argparse\n",
    "import glob\n",
    "import torchvision\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "#import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "# from utils import (\n",
    "#   read_data, \n",
    "#   input_setup, \n",
    "#   imsave,\n",
    "#   merge,\n",
    "#   modcrop)\n",
    "\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.disable_v2_behavior()\n",
    "from PIL import Image  # for loading images as YCbCr format\n",
    "\n",
    "import pylab\n",
    "from pylab import * \n",
    "from matplotlib import *\n",
    "import imageio\n",
    "import h5py\n",
    "import random\n",
    "from skimage import transform,data\n",
    "import cv2\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\users\\bmakh\\anaconda3\\envs\\upvideo\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Manipulating my video : split into images and cobvert to bmp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vidcap = cv2.VideoCapture('./farda1.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "    cv2.imwrite(\"./Dataset/Test/frame%d.bmp\" % count, image)     # save frame as JPEG file   \n",
    "    cv2.imwrite(\"./Dataset/JPEG/frame%d.jpg\" % count, image)\n",
    "    success,image = vidcap.read()\n",
    "    #print('Read a new frame: ', success)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show images of the video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './farda1.mp4'\n",
    "vid = imageio.get_reader(filename,  'ffmpeg')\n",
    "nums = [0, 1357]\n",
    "#Le Nombre max est 1357\n",
    "#x=0\n",
    "#for num in nums:\n",
    "    \n",
    " #   image = vid.get_data(num)\n",
    "  #  fig = pylab.figure()\n",
    "   # fig.suptitle('image #{}'.format(num), fontsize=20)\n",
    "    #pylab.imshow(image)\n",
    "for num in range(0, 50):\n",
    "    image = vid.get_data(num)\n",
    "    fig = pylab.figure()\n",
    "    fig.suptitle('img #{}'.format(num), fontsize=20)\n",
    "    pylab.imshow(image)\n",
    "    #t.save('./dataset/'+'t'+num, 'bmp')\n",
    "    pylab.savefig('./dataset/Test'+'test'+num, '.bmp')\n",
    "#    x=x+num\n",
    "#print(x)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Defining the architecture of my model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############      Save the model with its check points    ################\n",
    "def save(checkpoint_dir, step):\n",
    "    model_name = \"SRCNN.model\"\n",
    "    model_dir = \"%s_%s\" % (\"srcnn\", label_size)\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    saver.save(sess,\n",
    "                    os.path.join(checkpoint_dir, model_name),\n",
    "                    global_step=step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######      Load the model   and its checkpoints ########\n",
    "def load(checkpoint_dir):\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "    model_dir = \"%s_%s\" % (\"srcnn\", label_size)\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch=1500  #Number of epoch [15000]\n",
    "batch_size= 128 #The size of batch images [128]\")\n",
    "image_size= 33 #\"The size of image to use [33]\")\n",
    "label_size= 21 #\"The size of label to produce [21]\")\n",
    "learning_rate= 1e-4 #\"The learning rate of gradient descent algorithm [1e-4]\")\n",
    "c_dim= 1 #\"Dimension of image color. [1]\")\n",
    "scale= 3 #\"The size of scale factor for preprocessing input image [3]\")\n",
    "stride= 21 #\"The size of stride to apply input image [14]\")\n",
    "checkpoint_dir= \"output/checkpoint\" # \"Name of checkpoint directory [checkpoint]\")\n",
    "sample_dir= \"output/sample\"# \"Name of sample directory [sample]\")\n",
    "model= \"srcnn\" # \"Which module to use [srcnn]\")\n",
    "is_train= True #\"True for training, False for testing [True]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Manipulating the data (images to build and train the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it exist images or not (with the specified format)\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".bmp\", \".png\", \".jpg\", \".jpeg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modcrop(image, scale=3): #\n",
    "    \"\"\"\n",
    "      To scale down and up the original image, first thing to do is to have no remainder while scaling operation.\n",
    "\n",
    "      We need to find modulo of height (and width) and scale factor.\n",
    "      Then, subtract the modulo from height (and width) of original image size.\n",
    "      There would be no remainder even after scaling operation.\n",
    "      \"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        h, w, _ = image.shape\n",
    "        print(h)\n",
    "        h = h - np.mod(h, scale)\n",
    "        print(w)\n",
    "        w = w - np.mod(w, scale)\n",
    "        image = image[0:h, 0:w, :]\n",
    "    else:\n",
    "        h, w = image.shape\n",
    "        h = h - np.mod(h, scale)\n",
    "        w = w - np.mod(w, scale)\n",
    "        image = image[0:h, 0:w]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(path, is_grayscale=True):\n",
    "#   \"\"\"\n",
    "#   Read image using its path.\n",
    "#   Default value is gray-scale, and image is read by YCbCr format as the paper said.\n",
    "#   \"\"\"\n",
    "  if is_grayscale:\n",
    "        return imageio.imread(path, as_gray=True, pilmode='YCbCr').astype(np.float)#scipy.misc.imread(path, flatten=True, mode='YCbCr')\n",
    "#     img = cv2.imread(path)\n",
    "#     return img = img.flatten().astype(np.float)\n",
    "  else:\n",
    "        img = cv2.imread(path)  # Read input imgae\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert color format form BGR to RGB (OpenCV default is BGR).\n",
    "        img = img.astype(np.float)  # Convert ot float\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path, is_train, scale): # Args are \n",
    "  \n",
    "  if is_train:\n",
    "    image = imread(path, is_grayscale=False) #it was false\n",
    "    image = image[:,:,0] \n",
    "  \n",
    "    # Must be normalized\n",
    "    image = image / 255.\n",
    "\n",
    "    label_ = modcrop(image, scale)\n",
    "    input_ = scipy.ndimage.interpolation.zoom(label_, (1./scale), prefilter=False)\n",
    "    input_ = scipy.ndimage.interpolation.zoom(input_, (scale/1.), prefilter=False)\n",
    "\n",
    "    return input_, label_\n",
    "\n",
    "  else:\n",
    "    image = imread(path, is_grayscale=False)\n",
    "    # Must be normalized\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    #no color change using this resize method\n",
    "    image = transform.resize(image, (h//scale, w//scale, 3))\n",
    "    image = transform.resize(image, (h, w, 3))\n",
    "\n",
    "    input_dir = os.path.join(os.getcwd(),sample_dir,os.path.split(path)[-1])\n",
    "    scipy.misc.imsave(input_dir, image)\n",
    "\n",
    "    image = image / 255.\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imsave(image, path):\n",
    "    image = image * 255.\n",
    "    return scipy.misc.imsave(path, image)\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h*size[0], w*size[1], 1))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(sess, data, label=None): #Make input data as h5 file format\n",
    "  # Depending on 'is_train' (flag value), savepath would be changed.\n",
    "  if is_train:\n",
    "    savepath = os.path.join(os.getcwd(), 'output/checkpoint/train.h5')\n",
    "    with h5py.File(savepath, 'w') as hf: \n",
    "        hf.create_dataset('data', data=data)\n",
    "        hf.create_dataset('label', data=label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sess, dataset): # prepare the data for testing and training (test data and train data)\n",
    "    # args are dataset (test/ train)\n",
    "    # For train dataset output data would be ['.../t1.bmp', '.../t2.bmp', ..., '.../t99.bmp']\n",
    "    \n",
    "    if is_train:\n",
    "        data_dir = os.path.join(os.getcwd(), dataset)\n",
    "        data = [os.path.join(data_dir, x) for x in os.listdir(data_dir) if is_image_file(x)]\n",
    "    else:\n",
    "        data_dir = os.path.join(os.getcwd(), dataset)\n",
    "        data = [os.path.join(data_dir, x) for x in os.listdir(data_dir) if is_image_file(x)]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_setup(sess, is_train, image_size, label_size, scale, stride ):\n",
    "    \"\"\"\n",
    "      Read image files and make their sub-images and saved them as a h5 file format.\n",
    "       \"\"\"\n",
    "  # Load data path\n",
    "    if is_train:\n",
    "         data = prepare_data(sess, dataset=\"dataset/Train\")\n",
    "    else:\n",
    "         data = prepare_data(sess, dataset=\"dataset/Test\")\n",
    "    padding = abs(image_size - label_size) / 2 # 6\n",
    "    if is_train:\n",
    "        sub_input_sequence = []\n",
    "        sub_label_sequence = []\n",
    "        for i in range(len(data)):\n",
    "            input_, label_ = preprocess(data[i], is_train, scale) \n",
    "            if len(input_.shape) == 3:\n",
    "                h, w, _ = input_.shape\n",
    "            else:\n",
    "                h, w = input_.shape\n",
    "            for x in range(0, h-image_size+1, stride):\n",
    "                for y in range(0, w-image_size+1, stride):\n",
    "                        sub_input = input_[x:x+image_size, y:y+image_size] # [33 x 33]\n",
    "                        sub_label = label_[x+int(padding):x+int(padding)+label_size, y+int(padding):y+int(padding)+label_size] # [21 x 21]\n",
    "                        # Make channel value\n",
    "                        sub_input = sub_input.reshape([image_size, image_size, 1])  \n",
    "                        sub_label = sub_label.reshape([label_size, label_size, 1])\n",
    "                        sub_input_sequence.append(sub_input)\n",
    "                        sub_label_sequence.append(sub_label)\n",
    "\n",
    "        arrdata = np.asarray(sub_input_sequence) # [?, 33, 33, 1]\n",
    "        arrlabel = np.asarray(sub_label_sequence) # [?, 21, 21, 1]\n",
    "        make_data(sess, arrdata, arrlabel)\n",
    "\n",
    "    else:\n",
    "        nx = np.zeros(len(data), dtype=np.int)\n",
    "        ny = np.zeros(len(data), dtype=np.int)   \n",
    "        arr = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            image = preprocess(data[i], is_train, scale)\n",
    "            input_ = image\n",
    "            sub_input_sequence = [] \n",
    "            if len(input_.shape) == 3:\n",
    "                h, w, _ = input_.shape\n",
    "            else:\n",
    "                h, w = input_.shape\n",
    "\n",
    "            # Numbers of sub-images in height and width of image are needed to compute merge operation.\n",
    "            for x in range(0, h-image_size+1, stride):\n",
    "                nx[i] += 1; ny[i] = 0\n",
    "                for y in range(0, w-image_size+1, stride):\n",
    "                    ny[i] += 1\n",
    "                    sub_input = input_[x:x+image_size, y:y+image_size, :] # [33 x 33]\n",
    "                    sub_input = sub_input.reshape([image_size, image_size, 3])  \n",
    "                    sub_input_sequence.append(sub_input)\n",
    "\n",
    "            arr.append(np.asarray(sub_input_sequence))\n",
    "            sub_input_sequence.clear()\n",
    "#               \"\"\"\n",
    "#               len(sub_input_sequence) : the number of sub_input (33 x 33 x ch) in one image\n",
    "#               (sub_input_sequence[0]).shape : (33, 33, 1)\n",
    "#               \"\"\"\n",
    "    if not is_train:\n",
    "        return nx, ny, np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data formed by label and normal data \n",
    "# the data is saved in h5 format \n",
    "def read_data(path,is_train): # args are path , data and label  \n",
    "#     path: file path of desired file\n",
    "#     data: '.h5' file format that contains train data values\n",
    "#     label: '.h5' file format that contains train label values\n",
    "  with h5py.File(path, 'r') as hf:\n",
    "    data = np.array(hf.get('data'))\n",
    "    if is_train:\n",
    "        label = np.array(hf.get('label'))\n",
    "        return data, label\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start building the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model \n",
    "image_size=33\n",
    "label_size=21\n",
    "batch_size=128\n",
    "c_dim=1\n",
    "is_grayscale = (c_dim == 1)\n",
    "#x = tf.compat.v1.placeholder(tf.float32, shape=(1024, 1024))\n",
    "images = tf.placeholder(tf.float32, [None, image_size, image_size, c_dim], name='images')\n",
    "labels = tf.placeholder(tf.float32, [None, label_size, label_size, c_dim], name='labels')\n",
    "\n",
    "weights = {\n",
    "  'w1': tf.Variable(tf.random_normal([9, 9, 1, 64], stddev=1e-3), name='w1'),\n",
    "  'w2': tf.Variable(tf.random_normal([1, 1, 64, 32], stddev=1e-3), name='w2'),\n",
    "  'w3': tf.Variable(tf.random_normal([5, 5, 32, 1], stddev=1e-3), name='w3')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "  'b1': tf.Variable(tf.zeros([64]), name='b1'),\n",
    "  'b2': tf.Variable(tf.zeros([32]), name='b2'),\n",
    "  'b3': tf.Variable(tf.zeros([1]), name='b3')\n",
    "}\n",
    "def model(): \n",
    "    conv1 =  tf.nn.relu( tf.nn.conv2d(images, weights['w1'], strides=[1,1,1,1], padding='VALID') + biases['b1'])\n",
    "    conv2 =  tf.nn.relu( tf.nn.conv2d(conv1, weights['w2'], strides=[1,1,1,1], padding='VALID') + biases['b2'])\n",
    "    conv3 =  tf.nn.conv2d(conv2, weights['w3'], strides=[1,1,1,1], padding='VALID') + biases['b3']\n",
    "    return conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from output/checkpoint\\srcnn_21\\SRCNN.model-10500\n",
      " [*] Load SUCCESS\n",
      "Training...\n",
      "Epoch: [ 1], step: [10], time: [2.8752], loss: [0.47708213]\n",
      "Epoch: [ 1], step: [20], time: [5.4987], loss: [0.45070845]\n",
      "Epoch: [ 1], step: [30], time: [8.1490], loss: [0.33618870]\n",
      "Epoch: [ 1], step: [40], time: [10.8168], loss: [0.20626441]\n",
      "Epoch: [ 1], step: [50], time: [13.4138], loss: [0.29573232]\n",
      "Epoch: [ 1], step: [60], time: [16.0483], loss: [0.63185948]\n",
      "Epoch: [ 1], step: [70], time: [18.6979], loss: [0.46155137]\n",
      "Epoch: [ 2], step: [80], time: [21.4921], loss: [0.47090638]\n",
      "Epoch: [ 2], step: [90], time: [24.1182], loss: [0.30255875]\n",
      "Epoch: [ 2], step: [100], time: [26.7351], loss: [0.50271297]\n",
      "Epoch: [ 2], step: [110], time: [29.4065], loss: [0.42514205]\n",
      "Epoch: [ 2], step: [120], time: [32.0514], loss: [0.37401330]\n",
      "Epoch: [ 2], step: [130], time: [34.7281], loss: [0.13774389]\n",
      "Epoch: [ 2], step: [140], time: [37.3935], loss: [0.19046076]\n",
      "Epoch: [ 2], step: [150], time: [40.1126], loss: [0.29214835]\n",
      "Epoch: [ 3], step: [160], time: [42.7537], loss: [0.32804921]\n",
      "Epoch: [ 3], step: [170], time: [45.4004], loss: [0.40695745]\n",
      "Epoch: [ 3], step: [180], time: [48.0215], loss: [0.33666500]\n",
      "Epoch: [ 3], step: [190], time: [50.8013], loss: [0.54465461]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-dc3e6c90150a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch: [%2d], step: [%2d], time: [%4.4f], loss: [%.8f]\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmakh\\anaconda3\\envs\\upvideo\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 960\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    961\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmakh\\anaconda3\\envs\\upvideo\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1183\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1184\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmakh\\anaconda3\\envs\\upvideo\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1361\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmakh\\anaconda3\\envs\\upvideo\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1365\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmakh\\anaconda3\\envs\\upvideo\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1350\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1352\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmakh\\anaconda3\\envs\\upvideo\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1443\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1444\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1445\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    if is_train:\n",
    "        input_setup(sess, is_train, image_size, label_size, scale, stride )\n",
    "    else:\n",
    "        nx, ny, arr = input_setup(sess, is_train, image_size, label_size, scale, stride ) \n",
    "        print(np.shape(arr))\n",
    "\n",
    "    if is_train:     \n",
    "        data_dir = os.path.join('./{}'.format(checkpoint_dir), \"train.h5\")\n",
    "        train_data, train_label = read_data(data_dir,is_train=True)\n",
    "\n",
    "\n",
    "    pred=model()\n",
    "\n",
    "# Loss function (MSE)\n",
    "    loss = tf.reduce_mean(tf.square(labels - pred))\n",
    "\n",
    "    saver = tf.train.Saver()  \n",
    "\n",
    "    # Stochastic gradient descent with the standard backpropagation\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    if load(checkpoint_dir):\n",
    "        print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "        print(\" [!] Load failed...\")\n",
    "\n",
    "    if is_train:\n",
    "        print(\"Training...\")\n",
    "        for ep in range(epoch):\n",
    "        # Run by batch images\n",
    "            batch_idxs = len(train_data) // batch_size\n",
    "            for idx in range(0, batch_idxs):\n",
    "                batch_images = train_data[idx*batch_size : (idx+1)*batch_size]\n",
    "                batch_labels = train_label[idx*batch_size : (idx+1)*batch_size]\n",
    "                counter += 1\n",
    "                _, err = sess.run([train_op, loss], feed_dict={images: batch_images, labels: batch_labels})\n",
    "                if counter % 10 == 0:\n",
    "                    print(\"Epoch: [%2d], step: [%2d], time: [%4.4f], loss: [%.8f]\" % ((ep+1), counter, time.time()-start_time, err))\n",
    "                if counter % 500 == 0:\n",
    "                    save(checkpoint_dir, counter)\n",
    "\n",
    "    else:\n",
    "        print(\"Testing...\")\n",
    "        for i in range(len(arr)):\n",
    "            image = np.zeros((nx[i]*stride,ny[i]*stride,3))\n",
    "            for j in range(3):\n",
    "                result = pred.eval({images: arr[i][:,:,:,j].reshape([nx[i]*ny[i], image_size, image_size, 1])})\n",
    "                result = merge(result, [nx[i], ny[i]])\n",
    "                result = result.squeeze()\n",
    "                image[:, :, j] = result\n",
    "\n",
    "            image_path = os.path.join(os.getcwd(), sample_dir)\n",
    "            image_path = os.path.join(image_path, \"test_output%03d.png\"%i)\n",
    "            imsave(image, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
